
https://arxiv.org/pdf/1604.00289.pdf

## Abstract
machines do things, but not like humans. The use priors.
AI should:
1. Understand causality
2. Be grounded in physics and psychology
3. Rapid generalization

# 1. Introduction

neural networks cannot emulate a brain... which makes sense they are far too small.

statistical, pattern recognition, prediction first
alternative approach, model building.

prediction vs explination

humans are model based, although pattern recognition is also "support" the building of models.

"rich infrecnces from small amounts of training data"

reverse engineering humans is useful when humans are better than machines.

## 1.2 Overview of Key Ideas

### "start up software" aka priors
The earlier something exists, the more likely that it is part of future learning.  
earliest things are priors.

This paper focuses on intuitive physics and intuitive psychology

### learning
causual models

compositionability, when the larger thing (sentence?) is entirely defined by the ordering and meaning of its subparts (grammar, words) (https://plato.stanford.edu/entries/compositionality/)
"learning to learn", curiosity?

### reaction speed

humans can think quickly, react quickly in novel situations

building infrence based models is slow, but traditional pattern recognition techniques can help speed this up.

in humans, infrence based and pattern recognition are used both "colaboratively and cooperatively"??

### 2. "cognitive and neural inspiration in artificial inteligence"

historic behaviorism, mind as a blank notebook, alan turing

later, built on that and assumed that human knowledge and cognition could be thought of as symbols and operations.

neurons => paralell distributed processing (pdp)
    knowledge learned is "distributed across neurons"
    current state of the art deep learning is basically pdp with more compute and data
    **PDP is also compatible with model building**

end pg7
